{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "MicroJulia - Minimal GPT in Pure Julia",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": "<a href=\"https://colab.research.google.com/github/DavinciDreams/micro-julia/blob/main/microjulia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MicroJulia â€” A Minimal GPT in Pure Julia\n\nFaithful port of Karpathy's MicroJulia: the most atomic way to train a GPT.  \nEverything built from scratch: autograd, transformer, Adam optimizer.  \nNo external dependencies beyond Julia stdlib.\n\n**Architecture** (following GPT-2 with simplifications):\n- Custom scalar autograd engine (`Value` type)\n- Single-layer transformer with multi-head attention\n- RMSNorm (not LayerNorm), no biases, ReLU (not GELU)\n- KV cache for natural causal masking\n- Adam optimizer with linear LR decay\n- Temperature-controlled generation\n- Best-model checkpointing with validation loss tracking\n\nBased on: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Login & Setup\n",
    "\n",
    "This cell runs in **Python** to read your Colab secrets and set up credentials.\n",
    "\n",
    "1. Add secrets via the key icon (ðŸ”‘) in the left sidebar:\n",
    "   - `HF_TOKEN` â€” your HuggingFace access token\n",
    "   - `WANDB_KEY` â€” your Weights & Biases API key\n",
    "   - `HF_REPO` â€” your model repo (e.g. `LisaMegaWatts/MicroJulia`)\n",
    "2. Run cells 0â€“1 (login + install Julia, ~3-5 min)\n",
    "3. **Runtime â†’ Change runtime type â†’ Julia 1.10**\n",
    "4. Continue with the remaining Julia cells"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Login to HF + W&B and fetch training data â”€â”€\n",
    "!pip install -q wandb huggingface_hub\n",
    "\n",
    "import os, pathlib\n",
    "\n",
    "# â”€â”€ Read Colab secrets â”€â”€\n",
    "hf_token = \"\"\n",
    "wandb_key = \"\"\n",
    "hf_repo = \"\"\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    try: hf_token = userdata.get(\"HF_TOKEN\")\n",
    "    except Exception: pass\n",
    "    try: wandb_key = userdata.get(\"WANDB_KEY\")\n",
    "    except Exception: pass\n",
    "    try: hf_repo = userdata.get(\"HF_REPO\")\n",
    "    except Exception: pass\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# â”€â”€ Write tokens + repo to ~/.netrc (persists across kernel switch) â”€â”€\n",
    "netrc_path = pathlib.Path.home() / \".netrc\"\n",
    "netrc_lines = []\n",
    "if hf_token:\n",
    "    netrc_lines.extend([\"machine huggingface.co\", \"login hf\", \"password \" + hf_token, \"\"])\n",
    "if wandb_key:\n",
    "    netrc_lines.extend([\"machine api.wandb.ai\", \"login user\", \"password \" + wandb_key, \"\"])\n",
    "if hf_repo:\n",
    "    netrc_lines.extend([\"machine hf.repo\", \"login default\", \"password \" + hf_repo, \"\"])\n",
    "if netrc_lines:\n",
    "    netrc_path.write_text(chr(10).join(netrc_lines))\n",
    "    netrc_path.chmod(0o600)\n",
    "    print(f\"Credentials saved to {netrc_path}\")\n",
    "else:\n",
    "    print(\"No secrets found. Add HF_TOKEN, WANDB_KEY, HF_REPO via key icon.\")\n",
    "\n",
    "# â”€â”€ Fetch training data from repo â”€â”€\n",
    "DATA_FILE = \"/content/aristotle_rhetoric.txt\"\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    print(\"Fetching training data...\")\n",
    "    !git clone --depth 1 --filter=blob:none --sparse https://github.com/DavinciDreams/micro-julia.git /content/_microjulia 2>/dev/null || true\n",
    "    !cd /content/_microjulia && git sparse-checkout set data 2>/dev/null || true\n",
    "    src = \"/content/_microjulia/data/aristotle_rhetoric.txt\"\n",
    "    if os.path.exists(src):\n",
    "        import shutil\n",
    "        shutil.copy(src, DATA_FILE)\n",
    "        print(f\"Training data: {DATA_FILE} ({os.path.getsize(DATA_FILE)/1024:.0f} KB)\")\n",
    "    else:\n",
    "        print(\"Warning: could not fetch training data from repo.\")\n",
    "        print(\"Upload aristotle_rhetoric.txt manually to /content/\")\n",
    "else:\n",
    "    print(f\"Training data present: {DATA_FILE}\")\n",
    "\n",
    "# â”€â”€ Mount Google Drive for persistent checkpoints â”€â”€\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    drive_dir = \"/content/drive/MyDrive/MicroJulia\"\n",
    "    os.makedirs(drive_dir + \"/checkpoints\", exist_ok=True)\n",
    "    # Cache training data to Drive\n",
    "    if os.path.exists(DATA_FILE):\n",
    "        os.makedirs(drive_dir + \"/data\", exist_ok=True)\n",
    "        import shutil\n",
    "        shutil.copy(DATA_FILE, drive_dir + \"/data/aristotle_rhetoric.txt\")\n",
    "    print(f\"Drive mounted: {drive_dir}/\")\n",
    "except Exception as e:\n",
    "    print(f\"Drive mount skipped: {e}\")\n",
    "\n",
    "print(\"\\nDone! Now run the next cell to install Julia (~3-5 min).\")\n",
    "print(\"Then: Runtime > Change runtime type > Julia 1.10\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Install Julia Kernel\n\nThis cell downloads and installs Julia + IJulia. **Takes ~3-5 minutes** on first run.\n\n**After it finishes:**\n1. Go to **Runtime â†’ Change runtime type**\n2. You may see both \"Julia\" and \"Julia 1.10\" â€” pick **Julia 1.10**\n3. Continue running the cells below"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%%shell\nset -e\n\nJULIA_VERSION=\"1.10.5\"\nJULIA_MINOR=\"1.10\"\n\nif [ ! -d \"/usr/local/julia-${JULIA_VERSION}\" ]; then\n    echo \"Downloading Julia ${JULIA_VERSION}...\"\n    wget -q https://julialang-s3.julialang.org/bin/linux/x64/${JULIA_MINOR}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    tar xzf julia-${JULIA_VERSION}-linux-x86_64.tar.gz -C /usr/local/\n    rm julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    ln -sf /usr/local/julia-${JULIA_VERSION}/bin/julia /usr/local/bin/julia\n    echo \"Julia installed.\"\nelse\n    echo \"Julia already installed.\"\nfi\n\njulia -e '\n    using Pkg\n    Pkg.add(\"IJulia\")\n    Pkg.add(\"JSON3\")\n    using IJulia\n    installkernel(\"Julia\")\n'\n\necho \"\"\necho \"===========================================================\"\necho \"  Julia kernel installed!                                   \"\necho \"  Now: Runtime -> Change runtime type -> pick Julia 1.10       \"\necho \"  Then run the cells below.                              \"\necho \"===========================================================\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 1b. W&B + HuggingFace Helpers (Julia)\n\nW&B logging uses a persistent Python subprocess fed JSON lines from Julia.  \nHuggingFace helpers use `huggingface-cli` to push/pull checkpoints.  \nCredentials were saved to disk by the login cell above.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# â”€â”€ Ensure pip-installed binaries (huggingface-cli, wandb) are on PATH â”€â”€\nfor p in [\"/usr/local/bin\", joinpath(homedir(), \".local/bin\"), \"/root/.local/bin\"]\n    if isdir(p) && !occursin(p, get(ENV, \"PATH\", \"\"))\n        ENV[\"PATH\"] = p * \":\" * get(ENV, \"PATH\", \"\")\n    end\nend\n\n# â”€â”€ Read credentials + config from ~/.netrc (written by Python login cell) â”€â”€\nfunction load_netrc_tokens()\n    netrc = expanduser(\"~/.netrc\")\n    tokens = Dict{String,String}()\n    if !isfile(netrc)\n        return tokens\n    end\n    lines = readlines(netrc)\n    current_machine = \"\"\n    for line in lines\n        line = strip(line)\n        m = match(r\"^machine\\s+(\\S+)\", line)\n        if m !== nothing\n            current_machine = m.captures[1]\n        end\n        m = match(r\"^password\\s+(\\S+)\", line)\n        if m !== nothing\n            tokens[current_machine] = m.captures[1]\n        end\n    end\n    return tokens\nend\n\nnetrc_tokens = load_netrc_tokens()\n\n# W&B\nwandb_key = get(netrc_tokens, \"api.wandb.ai\", \"\")\nif !isempty(wandb_key)\n    ENV[\"WANDB_API_KEY\"] = wandb_key\n    println(\"W&B API key: found\")\nelse\n    println(\"W&B API key: not found (run Python login cell first)\")\nend\n\n# HuggingFace token\nhf_token = get(netrc_tokens, \"huggingface.co\", \"\")\nif !isempty(hf_token)\n    ENV[\"HF_TOKEN\"] = hf_token\n    println(\"HF token: found\")\nelse\n    println(\"HF token: not found (run Python login cell first)\")\nend\n\n# HuggingFace repo ID\nHF_REPO_ID = get(netrc_tokens, \"hf.repo\", \"\")\nif !isempty(HF_REPO_ID)\n    println(\"HF repo: \", HF_REPO_ID)\nelse\n    println(\"HF repo: not set (add HF_REPO to Colab secrets or set HF_REPO_ID manually)\")\nend",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# W&B logging via persistent Python subprocess\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nWANDB_PROJECT = \"microgpt-philosophy\"\nWANDB_RUN_ID = \"microgpt-\" * join(rand('a':'z', 6))\n\n# Write a tiny Python helper that reads JSON lines on stdin\nwrite(\"_wandb_log.py\", \"\"\"\nimport wandb, json, sys, os\nproject = os.environ.get(\"WANDB_PROJECT\", \"microgpt-philosophy\")\nrun_id = os.environ.get(\"WANDB_RUN_ID\", None)\nrun = wandb.init(project=project, id=run_id, resume=\"allow\",\n                 config={\"model\": \"microgpt\", \"architecture\": \"1-layer transformer\"})\nprint(f\"W&B run: {run.url}\", flush=True)\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    try:\n        data = json.loads(line)\n        wandb.log(data)\n    except Exception as e:\n        print(f\"wandb log error: {e}\", file=sys.stderr, flush=True)\nwandb.finish()\n\"\"\")\n\nwandb_proc = nothing\n\nfunction wandb_init()\n    global wandb_proc, WANDB_PROJECT, WANDB_RUN_ID\n    if !haskey(ENV, \"WANDB_API_KEY\") || isempty(ENV[\"WANDB_API_KEY\"])\n        println(\"W&B: skipped (no API key)\")\n        return\n    end\n    ENV[\"WANDB_PROJECT\"] = WANDB_PROJECT\n    ENV[\"WANDB_RUN_ID\"] = WANDB_RUN_ID\n    wandb_proc = open(`python3 _wandb_log.py`, \"r+\")\n    println(\"W&B: initialized ($WANDB_PROJECT / $WANDB_RUN_ID)\")\nend\n\nfunction wandb_log(; kwargs...)\n    global wandb_proc\n    wandb_proc === nothing && return\n    metrics = Dict(string(k) => v for (k, v) in kwargs)\n    try\n        println(wandb_proc, JSON3.write(metrics))\n        flush(wandb_proc)\n    catch e\n        println(\"W&B log error: $e\")\n    end\nend\n\nfunction wandb_finish()\n    global wandb_proc\n    wandb_proc === nothing && return\n    try close(wandb_proc) catch end\n    wandb_proc = nothing\n    println(\"W&B: run finished\")\nend\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# HuggingFace Hub helpers\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nfunction hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n    rp = isempty(remote_path) ? basename(local_path) : remote_path\n    run(`huggingface-cli upload $repo_id $local_path $rp`)\n    println(\"Pushed $local_path -> $repo_id/$rp\")\nend\n\nfunction hf_pull(repo_id::String, remote_path::String; local_dir::String=\"checkpoints\")\n    mkpath(local_dir)\n    run(`huggingface-cli download $repo_id $remote_path --local-dir $local_dir`)\n    println(\"Pulled $repo_id/$remote_path -> $local_dir/\")\nend\n\nfunction hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.json\")\n    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n    hf_push(repo_id, checkpoint_path)\nend\n\nfunction hf_create_repo(repo_id::String)\n    try\n        run(`huggingface-cli repo create $repo_id --type model`)\n        println(\"Created HF repo: $repo_id\")\n    catch\n        println(\"HF repo already exists or creation skipped: $repo_id\")\n    end\nend\n\nprintln(\"W&B + HuggingFace helpers defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Autograd Engine\n",
    "\n",
    "Scalar `Value` type with automatic differentiation.  \n",
    "Each operation creates a node in the computation graph with local gradients stored for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "using Pkg\nPkg.add(\"JSON3\")\n\nusing Random\nusing Printf\nusing JSON3\n\nRandom.seed!(42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Value\n",
    "    data::Float64\n",
    "    grad::Float64\n",
    "    _children::Vector{Value}\n",
    "    _local_grads::Vector{Float64}\n",
    "end\n",
    "\n",
    "Value(x::Real) = Value(Float64(x), 0.0, Value[], Float64[])\n",
    "\n",
    "# --- Core operations ---\n",
    "\n",
    "function Base.:+(a::Value, b::Value)\n",
    "    Value(a.data + b.data, 0.0, [a, b], [1.0, 1.0])\n",
    "end\n",
    "\n",
    "function Base.:*(a::Value, b::Value)\n",
    "    Value(a.data * b.data, 0.0, [a, b], [b.data, a.data])\n",
    "end\n",
    "\n",
    "function Base.:-(a::Value, b::Value)\n",
    "    Value(a.data - b.data, 0.0, [a, b], [1.0, -1.0])\n",
    "end\n",
    "\n",
    "function Base.:/(a::Value, b::Value)\n",
    "    Value(a.data / b.data, 0.0, [a, b], [1.0 / b.data, -a.data / b.data^2])\n",
    "end\n",
    "\n",
    "function Base.:^(a::Value, n::Real)\n",
    "    Value(a.data^n, 0.0, [a], [n * a.data^(n - 1)])\n",
    "end\n",
    "\n",
    "function Base.log(a::Value)\n",
    "    Value(log(a.data), 0.0, [a], [1.0 / a.data])\n",
    "end\n",
    "\n",
    "function Base.exp(a::Value)\n",
    "    e = exp(a.data)\n",
    "    Value(e, 0.0, [a], [e])\n",
    "end\n",
    "\n",
    "function relu(a::Value)\n",
    "    Value(max(0.0, a.data), 0.0, [a], [Float64(a.data > 0)])\n",
    "end\n",
    "\n",
    "# --- Scalar promotion ---\n",
    "\n",
    "Base.:+(a::Value, b::Real) = Value(a.data + b, 0.0, [a], [1.0])\n",
    "Base.:+(a::Real, b::Value) = Value(a + b.data, 0.0, [b], [1.0])\n",
    "Base.:*(a::Value, b::Real) = Value(a.data * b, 0.0, [a], [Float64(b)])\n",
    "Base.:*(a::Real, b::Value) = Value(a * b.data, 0.0, [b], [Float64(a)])\n",
    "Base.:-(a::Value, b::Real) = Value(a.data - b, 0.0, [a], [1.0])\n",
    "Base.:-(a::Real, b::Value) = Value(a - b.data, 0.0, [b], [-1.0])\n",
    "Base.:/(a::Value, b::Real) = Value(a.data / b, 0.0, [a], [1.0 / b])\n",
    "Base.:/(a::Real, b::Value) = Value(a / b.data, 0.0, [b], [-a / b.data^2])\n",
    "Base.:-(a::Value) = Value(-a.data, 0.0, [a], [-1.0])\n",
    "\n",
    "Base.zero(::Type{Value}) = Value(0.0)\n",
    "Base.isless(a::Value, b::Value) = a.data < b.data\n",
    "\n",
    "println(\"Value autograd type defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backward!(loss::Value)\n",
    "    topo = Value[]\n",
    "    visited = Set{UInt64}()\n",
    "    function build_topo(v)\n",
    "        id = objectid(v)\n",
    "        id in visited && return\n",
    "        push!(visited, id)\n",
    "        for child in v._children\n",
    "            build_topo(child)\n",
    "        end\n",
    "        push!(topo, v)\n",
    "    end\n",
    "    build_topo(loss)\n",
    "    loss.grad = 1.0\n",
    "    for v in reverse(topo)\n",
    "        for (child, lg) in zip(v._children, v._local_grads)\n",
    "            child.grad += lg * v.grad\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"backward! defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dataset â€” Load Training Data\n",
    "\n",
    "Training data (Aristotle's Rhetoric, 5,478 chunks) is fetched from the GitHub repo during setup.\n",
    "Falls back to built-in philosophy quotes if no file is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Load training data â”€â”€\n\nTRAINING_DATA = String[]\n\nSEARCH_PATHS = [\n    \"/content/aristotle_rhetoric.txt\",\n    \"/content/_microjulia/data/aristotle_rhetoric.txt\",\n    \"data/aristotle_rhetoric.txt\",\n    \"aristotle_rhetoric.txt\",\n    \"train.txt\",\n]\n\nfor candidate in SEARCH_PATHS\n    if isfile(candidate)\n        TRAINING_DATA = filter(!isempty, strip.(readlines(candidate)))\n        println(\"Loaded $(length(TRAINING_DATA)) docs from $candidate\")\n        break\n    end\nend\n\n# Also check Google Drive for cached data\nif isempty(TRAINING_DATA)\n    drive_path = \"/content/drive/MyDrive/MicroJulia/data/aristotle_rhetoric.txt\"\n    if isfile(drive_path)\n        TRAINING_DATA = filter(!isempty, strip.(readlines(drive_path)))\n        println(\"Loaded $(length(TRAINING_DATA)) docs from Google Drive\")\n    end\nend\n\n# Fall back to built-in quotes\nif isempty(TRAINING_DATA)\n    TRAINING_DATA = [\n        \"waste no more time arguing about what a good man should be\",\n        \"the happiness of your life depends upon the quality of your thoughts\",\n        \"you have power over your mind not outside events\",\n        \"very little is needed to make a happy life\",\n        \"the soul becomes dyed with the color of its thoughts\",\n        \"when you arise in the morning think of what a privilege it is to be alive\",\n        \"the best revenge is to be unlike him who performed the injury\",\n        \"accept the things to which fate binds you\",\n        \"if it is not right do not do it if it is not true do not say it\",\n        \"look well into thyself there is a source of strength\",\n        \"do every act of your life as though it were the very last act of your life\",\n        \"it is not death that a man should fear but never beginning to live\",\n        \"we suffer more often in imagination than in reality\",\n        \"true happiness is to enjoy the present without anxious dependence upon the future\",\n        \"it is not because things are difficult that we do not dare\",\n        \"it is because we do not dare that they are difficult\",\n        \"luck is what happens when preparation meets opportunity\",\n        \"begin at once to live and count each separate day as a separate life\",\n        \"the whole future lies in uncertainty live immediately\",\n        \"sometimes even to live is an act of courage\",\n        \"the unexamined life is not worth living\",\n        \"i know that i know nothing\",\n        \"wonder is the beginning of wisdom\",\n        \"knowing yourself is the beginning of all wisdom\",\n        \"happiness depends upon ourselves\",\n        \"it is the mark of an educated mind to entertain a thought without accepting it\",\n        \"we are what we repeatedly do excellence then is not an act but a habit\",\n        \"the whole is greater than the sum of its parts\",\n        \"nature does nothing in vain\",\n        \"the journey of a thousand miles begins with a single step\",\n        \"the mind is not a vessel to be filled but a fire to be kindled\",\n    ]\n    println(\"No training file found - using $(length(TRAINING_DATA)) built-in philosophy quotes\")\n    println(\"Searched paths:\")\n    for p in SEARCH_PATHS\n        status = isfile(p) ? \"found\" : \"not found\"\n        println(\"  $p -> $status\")\n    end\nend"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Neural Network Primitives + Helpers\n",
    "\n",
    "All operate on vectors of `Value` â€” linear layer, softmax, RMSNorm, plus helper functions for checkpoint support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function linear(x::Vector{Value}, w::Vector{Vector{Value}})\n",
    "    [sum(wi * xi for (wi, xi) in zip(wo, x)) for wo in w]\n",
    "end\n",
    "\n",
    "function softmax_v(logits::Vector{Value})\n",
    "    max_val = maximum(v.data for v in logits)\n",
    "    exps = [exp(v - max_val) for v in logits]\n",
    "    total = sum(exps)\n",
    "    [e / total for e in exps]\n",
    "end\n",
    "\n",
    "function rmsnorm(x::Vector{Value})\n",
    "    ms = sum(xi * xi for xi in x) / length(x)\n",
    "    scale = (ms + 1e-5) ^ (-0.5)\n",
    "    [xi * scale for xi in x]\n",
    "end\n",
    "\n",
    "# Helper: deterministic parameter key ordering\n",
    "function get_param_keys(n_layer::Int)\n",
    "    keys = [\"wte\", \"wpe\", \"lm_head\"]\n",
    "    for i in 0:n_layer-1\n",
    "        append!(keys, [\n",
    "            \"layer$i.attn_wq\", \"layer$i.attn_wk\", \"layer$i.attn_wv\", \"layer$i.attn_wo\",\n",
    "            \"layer$i.mlp_fc1\", \"layer$i.mlp_fc2\"\n",
    "        ])\n",
    "    end\n",
    "    return keys\n",
    "end\n",
    "\n",
    "# Helper: initialize weight matrices\n",
    "function init_matrix(nout::Int, nin::Int; std=0.08)\n",
    "    [[Value(randn() * std) for _ in 1:nin] for _ in 1:nout]\n",
    "end\n",
    "\n",
    "# Helper: flatten state_dict into params vector\n",
    "function flatten_params(state_dict, param_keys)\n",
    "    params = Value[]\n",
    "    for key in param_keys\n",
    "        for row in state_dict[key]\n",
    "            append!(params, row)\n",
    "        end\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "println(\"Neural network primitives + helpers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. GPT Forward Pass\n",
    "\n",
    "Processes one token at a time with KV cache for causal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gpt(token_id::Int, pos_id::Int,\n",
    "             keys::Vector{Vector{Vector{Value}}},\n",
    "             values::Vector{Vector{Vector{Value}}},\n",
    "             state_dict::Dict{String, Vector{Vector{Value}}},\n",
    "             n_layer::Int, n_head::Int, head_dim::Int)\n",
    "\n",
    "    tok_emb = state_dict[\"wte\"][token_id]\n",
    "    pos_emb = state_dict[\"wpe\"][pos_id]\n",
    "    x = [t + p for (t, p) in zip(tok_emb, pos_emb)]\n",
    "    x = rmsnorm(x)\n",
    "\n",
    "    for li in 0:n_layer-1\n",
    "        x_res = x\n",
    "        x = rmsnorm(x)\n",
    "        q = linear(x, state_dict[\"layer$(li).attn_wq\"])\n",
    "        k = linear(x, state_dict[\"layer$(li).attn_wk\"])\n",
    "        v = linear(x, state_dict[\"layer$(li).attn_wv\"])\n",
    "        push!(keys[li+1], k)\n",
    "        push!(values[li+1], v)\n",
    "\n",
    "        x_attn = Value[]\n",
    "        for h in 0:n_head-1\n",
    "            hs = h * head_dim + 1\n",
    "            he = hs + head_dim - 1\n",
    "            q_h = q[hs:he]\n",
    "            k_h = [ki[hs:he] for ki in keys[li+1]]\n",
    "            v_h = [vi[hs:he] for vi in values[li+1]]\n",
    "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in 1:head_dim) / sqrt(Float64(head_dim))\n",
    "                           for t in 1:length(k_h)]\n",
    "            attn_weights = softmax_v(attn_logits)\n",
    "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in 1:length(v_h))\n",
    "                        for j in 1:head_dim]\n",
    "            append!(x_attn, head_out)\n",
    "        end\n",
    "        x = linear(x_attn, state_dict[\"layer$(li).attn_wo\"])\n",
    "        x = [a + b for (a, b) in zip(x, x_res)]\n",
    "\n",
    "        x_res = x\n",
    "        x = rmsnorm(x)\n",
    "        x = linear(x, state_dict[\"layer$(li).mlp_fc1\"])\n",
    "        x = [relu(xi) for xi in x]\n",
    "        x = linear(x, state_dict[\"layer$(li).mlp_fc2\"])\n",
    "        x = [a + b for (a, b) in zip(x, x_res)]\n",
    "    end\n",
    "\n",
    "    logits = linear(x, state_dict[\"lm_head\"])\n",
    "    return logits\n",
    "end\n",
    "\n",
    "println(\"GPT forward pass defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Checkpoint Save/Load\n",
    "\n",
    "Save and load model weights + optimizer state as JSON.  \n",
    "Checkpoints saved to Google Drive persist across Colab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_checkpoint(path::String, state_dict, param_keys, uchars, hyperparams;\n",
    "                         m_buf=nothing, v_buf=nothing, step::Int=0,\n",
    "                         lr::Float64=0.01, b1::Float64=0.85, b2::Float64=0.99,\n",
    "                         best_val_loss::Float64=Inf,\n",
    "                         train_losses::Vector{Float64}=Float64[],\n",
    "                         val_losses::Vector{Float64}=Float64[],\n",
    "                         total_steps::Int=0, num_steps_target::Int=0)\n",
    "\n",
    "    sd_data = Dict{String,Any}()\n",
    "    for k in param_keys\n",
    "        sd_data[k] = [[v.data for v in row] for row in state_dict[k]]\n",
    "    end\n",
    "\n",
    "    checkpoint = Dict{String,Any}(\n",
    "        \"uchars\" => [string(c) for c in uchars],\n",
    "        \"hyperparams\" => hyperparams,\n",
    "        \"state_dict\" => sd_data,\n",
    "        \"optimizer\" => Dict{String,Any}(\n",
    "            \"m_buf\" => m_buf === nothing ? Float64[] : collect(m_buf),\n",
    "            \"v_buf\" => v_buf === nothing ? Float64[] : collect(v_buf),\n",
    "            \"step\" => step,\n",
    "            \"lr\" => lr,\n",
    "            \"beta1\" => b1,\n",
    "            \"beta2\" => b2\n",
    "        ),\n",
    "        \"training\" => Dict{String,Any}(\n",
    "            \"best_val_loss\" => best_val_loss,\n",
    "            \"train_losses\" => train_losses,\n",
    "            \"val_losses\" => val_losses,\n",
    "            \"total_steps_completed\" => total_steps,\n",
    "            \"num_steps_target\" => num_steps_target\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mkpath(dirname(path))\n",
    "    open(path, \"w\") do f\n",
    "        JSON3.write(f, checkpoint)\n",
    "    end\n",
    "    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n",
    "    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\n",
    "end\n",
    "\n",
    "function load_checkpoint(path::String)\n",
    "    println(\"Loading checkpoint from $path ...\")\n",
    "    raw = JSON3.read(read(path, String))\n",
    "\n",
    "    uchars = [only(String(s)) for s in raw[\"uchars\"]]\n",
    "    BOS = length(uchars) + 1\n",
    "    vocab_size = BOS\n",
    "\n",
    "    hp = raw[\"hyperparams\"]\n",
    "    n_layer = Int(hp[\"n_layer\"])\n",
    "    n_embd = Int(hp[\"n_embd\"])\n",
    "    block_size = Int(hp[\"block_size\"])\n",
    "    n_head = Int(hp[\"n_head\"])\n",
    "    head_dim = n_embd Ã· n_head\n",
    "\n",
    "    state_dict = Dict{String, Vector{Vector{Value}}}()\n",
    "    for (key, matrix) in pairs(raw[\"state_dict\"])\n",
    "        state_dict[string(key)] = [[Value(Float64(v)) for v in row] for row in matrix]\n",
    "    end\n",
    "\n",
    "    opt = raw[\"optimizer\"]\n",
    "    m_buf = Float64.(collect(opt[\"m_buf\"]))\n",
    "    v_buf = Float64.(collect(opt[\"v_buf\"]))\n",
    "    step = Int(opt[\"step\"])\n",
    "    lr = Float64(opt[\"lr\"])\n",
    "    b1 = Float64(opt[\"beta1\"])\n",
    "    b2 = Float64(opt[\"beta2\"])\n",
    "\n",
    "    trn = raw[\"training\"]\n",
    "    best_val_loss = Float64(trn[\"best_val_loss\"])\n",
    "    train_losses = Float64.(collect(trn[\"train_losses\"]))\n",
    "    val_losses = Float64.(collect(trn[\"val_losses\"]))\n",
    "    total_steps = Int(trn[\"total_steps_completed\"])\n",
    "    num_steps_target = Int(trn[\"num_steps_target\"])\n",
    "\n",
    "    println(\"  vocab=$vocab_size, embd=$n_embd, layers=$n_layer, step=$step\")\n",
    "\n",
    "    return (;\n",
    "        state_dict, uchars, BOS, vocab_size,\n",
    "        n_layer, n_embd, block_size, n_head, head_dim,\n",
    "        m_buf, v_buf, step, lr, b1, b2,\n",
    "        best_val_loss, train_losses, val_losses,\n",
    "        total_steps, num_steps_target\n",
    "    )\n",
    "end\n",
    "\n",
    "println(\"Checkpoint save/load defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup â€” Dataset, Tokenizer, Parameters\n",
    "\n",
    "Character-level tokenizer with a BOS token. 90/10 train/val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Dataset with train/val split â”€â”€\ndocs = shuffle(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\nprintln(\"train: $(length(train_docs)) docs, val: $(length(val_docs)) docs\")\n\n# â”€â”€ Tokenizer â”€â”€\nuchars = sort(unique(join(docs)))\nBOS = length(uchars) + 1\nvocab_size = BOS\nprintln(\"vocab size: $vocab_size ($(length(uchars)) chars + BOS)\")\n\n# â”€â”€ Hyperparameters â”€â”€\nn_layer    = 1\nn_embd     = 16\nblock_size = 256\nn_head     = 4\nhead_dim   = n_embd Ã· n_head\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# â”€â”€ Initialize parameters â”€â”€\nstate_dict = Dict{String, Vector{Vector{Value}}}()\nstate_dict[\"wte\"]     = init_matrix(vocab_size, n_embd)\nstate_dict[\"wpe\"]     = init_matrix(block_size, n_embd)\nstate_dict[\"lm_head\"] = init_matrix(vocab_size, n_embd)\nfor i in 0:n_layer-1\n    state_dict[\"layer$i.attn_wq\"]  = init_matrix(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wk\"]  = init_matrix(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wv\"]  = init_matrix(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wo\"]  = init_matrix(n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc1\"]  = init_matrix(4 * n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc2\"]  = init_matrix(n_embd, 4 * n_embd)\nend\n\nparam_keys = get_param_keys(n_layer)\nparams = flatten_params(state_dict, param_keys)\nprintln(\"num params: $(length(params))\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Loop with Validation + Best-Model Checkpointing\n",
    "\n",
    "Adam optimizer with linear LR decay.  \n",
    "Validates every 50 steps, saves `best_model.json` when val loss improves.  \n",
    "Periodic checkpoints every 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for doc in val_docs\n",
    "        tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n",
    "        n = min(block_size, length(tokens) - 1)\n",
    "        kv_keys = [Vector{Vector{Value}}() for _ in 1:n_layer]\n",
    "        kv_vals = [Vector{Vector{Value}}() for _ in 1:n_layer]\n",
    "        for pos in 1:n\n",
    "            token_id = tokens[pos]\n",
    "            target_id = tokens[pos + 1]\n",
    "            logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n",
    "            probs = softmax_v(logits)\n",
    "            total_loss += -log(probs[target_id]).data\n",
    "            total_tokens += 1\n",
    "        end\n",
    "    end\n",
    "    return total_loss / max(total_tokens, 1)\n",
    "end\n",
    "\n",
    "println(\"compute_val_loss defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Adam optimizer buffers â”€â”€\nlr, b1, b2, eps = 0.01, 0.85, 0.99, 1e-8\nm_buf = zeros(length(params))\nv_buf = zeros(length(params))\n\nbest_val_loss = Inf\n\n# â”€â”€ Checkpoint paths (local + Drive for persistence) â”€â”€\nLOCAL_CKPT = \"checkpoints\"\nDRIVE_CKPT = \"/content/drive/MyDrive/MicroJulia/checkpoints\"\nmkpath(LOCAL_CKPT)\n\nfunction save_to_drive(local_path::String)\n    if isdir(DRIVE_CKPT)\n        cp(local_path, joinpath(DRIVE_CKPT, basename(local_path)), force=true)\n    end\nend\n\n\ntrain_loss_history = Float64[]\nval_loss_history = Float64[]\n\n# â”€â”€ Initialize W&B logging (if API key is set) â”€â”€\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\n# â”€â”€ Training loop â”€â”€\n# Dynamic steps: ~3 epochs over training data, min 1000, max 50000\nNUM_EPOCHS = 3\nnum_steps = clamp(NUM_EPOCHS * length(train_docs), 1000, 50000)\nprintln(\"--- training $num_steps steps (~$(NUM_EPOCHS) epochs over $(length(train_docs)) docs) ---\")\nt_start = time()\n\nfor step in 1:num_steps\n    doc = train_docs[mod1(step, length(train_docs))]\n    tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n    n = min(block_size, length(tokens) - 1)\n\n    # Forward pass\n    kv_keys = [Vector{Vector{Value}}() for _ in 1:n_layer]\n    kv_vals = [Vector{Vector{Value}}() for _ in 1:n_layer]\n    losses = Value[]\n    for pos in 1:n\n        token_id  = tokens[pos]\n        target_id = tokens[pos + 1]\n        logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n        probs  = softmax_v(logits)\n        push!(losses, -log(probs[target_id]))\n    end\n    loss = (1 / n) * sum(losses)\n    push!(train_loss_history, loss.data)\n\n    # Backward pass\n    backward!(loss)\n\n    # Adam update with linear LR decay\n    lr_t = lr * (1 - (step - 1) / num_steps)\n    for (i, p) in enumerate(params)\n        m_buf[i] = b1 * m_buf[i] + (1 - b1) * p.grad\n        v_buf[i] = b2 * v_buf[i] + (1 - b2) * p.grad^2\n        m_hat = m_buf[i] / (1 - b1^step)\n        v_hat = v_buf[i] / (1 - b2^step)\n        p.data -= lr_t * m_hat / (sqrt(v_hat) + eps)\n        p.grad = 0.0\n    end\n\n    # Validate + checkpoint every 50 steps\n    if step % 50 == 0\n        val_loss = compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n        push!(val_loss_history, val_loss)\n        elapsed = time() - t_start\n\n        # Log to W&B\n        wandb_log(; step=step, train_loss=loss.data, val_loss=val_loss, lr=lr_t)\n\n        improved = \"\"\n        if val_loss < best_val_loss\n            best_val_loss = val_loss\n            save_checkpoint(\"checkpoints/best_model.json\", state_dict, param_keys, uchars, hyperparams;\n                m_buf=m_buf, v_buf=v_buf, step=step,\n                lr=lr, b1=b1, b2=b2,\n                best_val_loss=best_val_loss,\n                train_losses=train_loss_history, val_losses=val_loss_history,\n                total_steps=step, num_steps_target=num_steps)\n            improved = \" << new best!\"\n        end\n\n        @printf(\"step %4d / %4d | train %.4f | val %.4f | %.1fs%s\\n\",\n                step, num_steps, loss.data, val_loss, elapsed, improved)\n    elseif step % 10 == 0\n        elapsed = time() - t_start\n        @printf(\"step %4d / %4d | train %.4f | %.1fs\\n\", step, num_steps, loss.data, elapsed)\n    end\n\n    # Periodic checkpoint every 200 steps\n    if step % 200 == 0\n        save_checkpoint(\"checkpoints/checkpoint_step$(step).json\", state_dict, param_keys, uchars, hyperparams;\n            m_buf=m_buf, v_buf=v_buf, step=step,\n            lr=lr, b1=b1, b2=b2,\n            best_val_loss=best_val_loss,\n            train_losses=train_loss_history, val_losses=val_loss_history,\n            total_steps=step, num_steps_target=num_steps)\n    end\nend\n\nelapsed = time() - t_start\n@printf(\"\\ntraining complete in %.1f seconds\\n\", elapsed)\n\n# Finish W&B run\nwandb_finish()\n\n# Final save\nsave_checkpoint(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    m_buf=m_buf, v_buf=v_buf, step=num_steps,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=num_steps, num_steps_target=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Inference â€” Hallucinated Philosophy\n",
    "\n",
    "Generate new philosophy-like text using temperature-controlled sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_text(state_dict, uchars, BOS, n_layer, n_head, head_dim, block_size;\n",
    "                       temperature=0.8, max_tokens=128)\n",
    "    kv_keys = [Vector{Vector{Value}}() for _ in 1:n_layer]\n",
    "    kv_vals = [Vector{Vector{Value}}() for _ in 1:n_layer]\n",
    "    token_id = BOS\n",
    "    sample = Char[]\n",
    "    limit = min(max_tokens, block_size)\n",
    "    for pos in 1:limit\n",
    "        logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n",
    "        scaled = [l / temperature for l in logits]\n",
    "        probs = softmax_v(scaled)\n",
    "        weights = [p.data for p in probs]\n",
    "        r = rand()\n",
    "        cum = 0.0\n",
    "        token_id = 1\n",
    "        for (idx, w) in enumerate(weights)\n",
    "            cum += w\n",
    "            if r <= cum\n",
    "                token_id = idx\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        token_id == BOS && break\n",
    "        push!(sample, uchars[token_id])\n",
    "    end\n",
    "    return String(sample)\n",
    "end\n",
    "\n",
    "println(\"--- inference (hallucinated philosophy) ---\")\n",
    "for i in 1:20\n",
    "    text = generate_text(state_dict, uchars, BOS, n_layer, n_head, head_dim, block_size; temperature=0.8)\n",
    "    @printf(\"sample %2d: %s\\n\", i, text)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8a. Push Model to HuggingFace Hub\n\nPush your trained checkpoint to HuggingFace for persistence across Colab sessions.  \nSet `HF_REPO_ID` in the login cell above (e.g. `\"yourusername/microgpt-philosophy\"`).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Push checkpoint to HuggingFace Hub\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    # Create repo if it doesn't exist yet\n    hf_create_repo(HF_REPO_ID)\n\n    # Push best model checkpoint\n    if isfile(\"checkpoints/best_model.json\")\n        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.json\")\n    else\n        println(\"No best_model.json found â€” train first!\")\n    end\n\n    # Also push final model if it exists\n    if isfile(\"checkpoints/final_model.json\")\n        hf_push(HF_REPO_ID, \"checkpoints/final_model.json\")\n    end\n\n    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8b. Pull Checkpoint from HuggingFace Hub\n\nDownload a previously pushed checkpoint to resume training in a new Colab session.  \nRun this before the Resume Training cell below.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Pull checkpoint from HuggingFace to resume training\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    mkpath(\"checkpoints\")\n    hf_pull(HF_REPO_ID, \"best_model.json\"; local_dir=\"checkpoints\")\n    println(\"\\nReady to resume from checkpoints/best_model.json\")\n    println(\"Run the 'Resume Training' cell below.\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Resume Training from Checkpoint\n",
    "\n",
    "Load a saved checkpoint and continue training for more steps.  \n",
    "Skip this cell if you're training from scratch above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Load checkpoint â”€â”€\n# Change the path to load a different checkpoint\nRESUME_FROM = \"checkpoints/best_model.json\"\nEXTRA_STEPS = clamp(length(train_docs), 500, 25000)  # ~1 extra epoch\n\nckpt = load_checkpoint(RESUME_FROM)\nstate_dict = ckpt.state_dict\nuchars = ckpt.uchars\nBOS = ckpt.BOS\nn_layer = ckpt.n_layer\nn_embd = ckpt.n_embd\nblock_size = ckpt.block_size\nn_head = ckpt.n_head\nhead_dim = ckpt.head_dim\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# Reconstruct dataset and split\ndocs = shuffle(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\n\nparam_keys = get_param_keys(n_layer)\nparams = flatten_params(state_dict, param_keys)\n\n# Restore optimizer\nlr = ckpt.lr; b1 = ckpt.b1; b2 = ckpt.b2; eps = 1e-8\nm_buf = length(ckpt.m_buf) == length(params) ? copy(ckpt.m_buf) : zeros(length(params))\nv_buf = length(ckpt.v_buf) == length(params) ? copy(ckpt.v_buf) : zeros(length(params))\n\nstart_step = ckpt.step + 1\nend_step = ckpt.step + EXTRA_STEPS\nbest_val_loss = ckpt.best_val_loss\ntrain_loss_history = copy(ckpt.train_losses)\nval_loss_history = copy(ckpt.val_losses)\n\n# â”€â”€ Initialize W&B logging (if API key is set) â”€â”€\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nprintln(\"\\nResuming from step $(ckpt.step) -> training to step $end_step\")\nprintln(\"Best val loss so far: $(round(best_val_loss, digits=4))\")\nt_start = time()\n\nfor step in start_step:end_step\n    doc = train_docs[mod1(step, length(train_docs))]\n    tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n    n = min(block_size, length(tokens) - 1)\n\n    kv_keys = [Vector{Vector{Value}}() for _ in 1:n_layer]\n    kv_vals = [Vector{Vector{Value}}() for _ in 1:n_layer]\n    losses = Value[]\n    for pos in 1:n\n        token_id  = tokens[pos]\n        target_id = tokens[pos + 1]\n        logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n        probs  = softmax_v(logits)\n        push!(losses, -log(probs[target_id]))\n    end\n    loss = (1 / n) * sum(losses)\n    push!(train_loss_history, loss.data)\n\n    backward!(loss)\n\n    lr_t = lr * (1 - (step - 1) / end_step)\n    for (i, p) in enumerate(params)\n        m_buf[i] = b1 * m_buf[i] + (1 - b1) * p.grad\n        v_buf[i] = b2 * v_buf[i] + (1 - b2) * p.grad^2\n        m_hat = m_buf[i] / (1 - b1^step)\n        v_hat = v_buf[i] / (1 - b2^step)\n        p.data -= lr_t * m_hat / (sqrt(v_hat) + eps)\n        p.grad = 0.0\n    end\n\n    if step % 50 == 0\n        val_loss = compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n        push!(val_loss_history, val_loss)\n        elapsed = time() - t_start\n\n        # Log to W&B\n        wandb_log(; step=step, train_loss=loss.data, val_loss=val_loss, lr=lr_t)\n\n        improved = \"\"\n        if val_loss < best_val_loss\n            best_val_loss = val_loss\n            save_checkpoint(\"checkpoints/best_model.json\", state_dict, param_keys, uchars, hyperparams;\n                m_buf=m_buf, v_buf=v_buf, step=step,\n                lr=lr, b1=b1, b2=b2,\n                best_val_loss=best_val_loss,\n                train_losses=train_loss_history, val_losses=val_loss_history,\n                total_steps=step, num_steps_target=end_step)\n            improved = \" << new best!\"\n        end\n        @printf(\"step %4d / %4d | train %.4f | val %.4f | %.1fs%s\\n\",\n                step, end_step, loss.data, val_loss, elapsed, improved)\n    elseif step % 10 == 0\n        elapsed = time() - t_start\n        @printf(\"step %4d / %4d | train %.4f | %.1fs\\n\", step, end_step, loss.data, elapsed)\n    end\nend\n\nelapsed = time() - t_start\n@printf(\"\\nresume training complete in %.1f seconds\\n\", elapsed)\n\n# Finish W&B run\nwandb_finish()\n\nsave_checkpoint(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    m_buf=m_buf, v_buf=v_buf, step=end_step,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=end_step, num_steps_target=end_step)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Download Checkpoint\n",
    "\n",
    "Download the best model checkpoint to use with the inference server.  \n",
    "In Colab, use the Files panel (left sidebar) to download, or copy to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved checkpoints\n",
    "if isdir(\"checkpoints\")\n",
    "    files = readdir(\"checkpoints\")\n",
    "    println(\"Saved checkpoints:\")\n",
    "    for f in files\n",
    "        path = joinpath(\"checkpoints\", f)\n",
    "        size_kb = round(filesize(path) / 1024, digits=1)\n",
    "        println(\"  $path ($(size_kb) KB)\")\n",
    "    end\n",
    "else\n",
    "    println(\"No checkpoints directory found. Train first!\")\n",
    "end"
   ]
  }
 ]
}