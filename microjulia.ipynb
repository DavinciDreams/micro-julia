{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "MicroJulia - Minimal GPT in Pure Julia",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "julia",
   "display_name": "Julia"
  },
  "language_info": {
   "name": "julia"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": "<a href=\"https://colab.research.google.com/github/DavinciDreams/micro-julia/blob/main/microjulia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MicroJulia â€” A Minimal GPT in Pure Julia\n\nFaithful port of Karpathy's MicroJulia: the most atomic way to train a GPT.  \nEverything built from scratch: autograd, transformer, Adam optimizer.  \nNo external dependencies beyond Julia stdlib.\n\n**Architecture** (following GPT-2 with simplifications):\n- Custom scalar autograd engine (`Value` type)\n- Single-layer transformer with multi-head attention\n- RMSNorm (not LayerNorm), no biases, ReLU (not GELU)\n- KV cache for natural causal masking\n- Adam optimizer with linear LR decay\n- Temperature-controlled generation\n- Best-model checkpointing with validation loss tracking\n\nBased on: https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Login to HuggingFace & Weights & Biases\n\nThe notebook opens in **Python** so you can log in and install Julia first.\n\n1. Add secrets via the key icon (ðŸ”‘) in the left sidebar:\n   - `HF_TOKEN` â€” your HuggingFace access token\n   - `WANDB_KEY` â€” your Weights & Biases API key\n   - `HF_REPO` â€” your model repo (e.g. `LisaMegaWatts/JuliaGPT`)\n2. Run this cell (login + fetch data)\n3. Run the next cell (install Julia, ~3-5 min)\n4. **Runtime â†’ Change runtime type â†’ Julia 1.10**\n5. Continue with the remaining cells"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# â”€â”€ Login to HF + W&B and fetch training data (runs in Python before kernel switch) â”€â”€\n!pip install -q wandb huggingface_hub\n\nimport os, pathlib\n\n# â”€â”€ Read Colab secrets â”€â”€\nhf_token = \"\"\nwandb_key = \"\"\nhf_repo = \"\"\ntry:\n    from google.colab import userdata\n    try:\n        hf_token = userdata.get(\"HF_TOKEN\")\n    except Exception:\n        pass\n    try:\n        wandb_key = userdata.get(\"WANDB_KEY\")\n    except Exception:\n        pass\n    try:\n        hf_repo = userdata.get(\"HF_REPO\")\n    except Exception:\n        pass\nexcept ImportError:\n    pass\n\n# â”€â”€ HuggingFace login â”€â”€\nif hf_token:\n    os.environ[\"HF_TOKEN\"] = hf_token\n    !huggingface-cli login --token {hf_token} --add-to-git-credential\n    print(\"HuggingFace: logged in\")\nelse:\n    print(\"HuggingFace: skipped (add HF_TOKEN to Colab secrets via key icon)\")\n\n# â”€â”€ W&B login â”€â”€\nif wandb_key:\n    os.environ[\"WANDB_API_KEY\"] = wandb_key\n    !wandb login {wandb_key}\n    print(\"W&B: logged in\")\nelse:\n    print(\"W&B: skipped (add WANDB_KEY to Colab secrets via key icon)\")\n\nif hf_repo:\n    print(f\"HF repo: {hf_repo}\")\nelse:\n    print(\"HF repo: not set (add HF_REPO to Colab secrets, e.g. LisaMegaWatts/JuliaGPT)\")\n\n# â”€â”€ Write tokens + repo to ~/.netrc so they survive the Julia kernel switch â”€â”€\nnetrc_path = pathlib.Path.home() / \".netrc\"\nnetrc_lines = []\nif hf_token:\n    netrc_lines.extend([\"machine huggingface.co\", \"login hf\", \"password \" + hf_token, \"\"])\nif wandb_key:\n    netrc_lines.extend([\"machine api.wandb.ai\", \"login user\", \"password \" + wandb_key, \"\"])\nif hf_repo:\n    netrc_lines.extend([\"machine hf.repo\", \"login default\", \"password \" + hf_repo, \"\"])\nif netrc_lines:\n    netrc_path.write_text(chr(10).join(netrc_lines))\n    netrc_path.chmod(0o600)\n    print(f\"Config saved to {netrc_path} (persists across kernel switch)\")\n\n# â”€â”€ Fetch training data from repo â”€â”€\nREPO_URL = \"https://github.com/DavinciDreams/micro-julia.git\"\nDATA_FILE = \"/content/aristotle_rhetoric.txt\"\n\nif not os.path.exists(DATA_FILE):\n    print(\"Fetching training data from repo...\")\n    !git clone --depth 1 --filter=blob:none --sparse {REPO_URL} /content/_microjulia 2>/dev/null\n    !cd /content/_microjulia && git sparse-checkout set data 2>/dev/null\n    if os.path.exists(\"/content/_microjulia/data/aristotle_rhetoric.txt\"):\n        import shutil\n        shutil.copy(\"/content/_microjulia/data/aristotle_rhetoric.txt\", DATA_FILE)\n        size_kb = os.path.getsize(DATA_FILE) / 1024\n        print(f\"Training data ready: {DATA_FILE} ({size_kb:.0f} KB)\")\n    else:\n        print(\"Warning: data/aristotle_rhetoric.txt not found in repo\")\nelse:\n    print(f\"Training data already present: {DATA_FILE}\")\n\nprint(\"Done! Now install Julia (next cell), then switch kernel to Julia.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Install Julia Kernel\n\nThis cell downloads and installs Julia + IJulia. **Takes ~3-5 minutes** on first run.\n\n**After it finishes:**\n1. Go to **Runtime â†’ Change runtime type**\n2. You may see both \"Julia\" and \"Julia 1.10\" â€” pick **Julia 1.10**\n3. Continue running the cells below"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%%shell\nset -e\n\nJULIA_VERSION=\"1.10.5\"\nJULIA_MINOR=\"1.10\"\n\nif [ ! -d \"/usr/local/julia-${JULIA_VERSION}\" ]; then\n    echo \"Downloading Julia ${JULIA_VERSION}...\"\n    wget -q https://julialang-s3.julialang.org/bin/linux/x64/${JULIA_MINOR}/julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    tar xzf julia-${JULIA_VERSION}-linux-x86_64.tar.gz -C /usr/local/\n    rm julia-${JULIA_VERSION}-linux-x86_64.tar.gz\n    ln -sf /usr/local/julia-${JULIA_VERSION}/bin/julia /usr/local/bin/julia\n    echo \"Julia installed.\"\nelse\n    echo \"Julia already installed.\"\nfi\n\njulia -e '\n    using Pkg\n    Pkg.add(\"IJulia\")\n    Pkg.add(\"JSON3\")\n    using IJulia\n    installkernel(\"Julia\")\n'\n\necho \"\"\necho \"===========================================================\"\necho \"  Julia kernel installed!                                   \"\necho \"  Now: Runtime -> Change runtime type -> pick Julia 1.10       \"\necho \"  Then run the cells below.                              \"\necho \"===========================================================\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 1b. W&B + HuggingFace Helpers (Julia)\n\nW&B logging uses a persistent Python subprocess fed JSON lines from Julia.  \nHuggingFace helpers use `huggingface-cli` to push/pull checkpoints.  \nCredentials were saved to disk by the login cell above.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Read credentials + config from ~/.netrc (written by Python login cell) â”€â”€\n",
    "function load_netrc_tokens()\n",
    "    netrc = expanduser(\"~/.netrc\")\n",
    "    tokens = Dict{String,String}()\n",
    "    if !isfile(netrc)\n",
    "        return tokens\n",
    "    end\n",
    "    lines = readlines(netrc)\n",
    "    current_machine = \"\"\n",
    "    for line in lines\n",
    "        line = strip(line)\n",
    "        m = match(r\"^machine\\s+(\\S+)\", line)\n",
    "        if m !== nothing\n",
    "            current_machine = m.captures[1]\n",
    "        end\n",
    "        m = match(r\"^password\\s+(\\S+)\", line)\n",
    "        if m !== nothing\n",
    "            tokens[current_machine] = m.captures[1]\n",
    "        end\n",
    "    end\n",
    "    return tokens\n",
    "end\n",
    "\n",
    "netrc_tokens = load_netrc_tokens()\n",
    "\n",
    "# W&B\n",
    "wandb_key = get(netrc_tokens, \"api.wandb.ai\", \"\")\n",
    "if !isempty(wandb_key)\n",
    "    ENV[\"WANDB_API_KEY\"] = wandb_key\n",
    "    println(\"W&B API key: found\")\n",
    "else\n",
    "    println(\"W&B API key: not found (run Python login cell first)\")\n",
    "end\n",
    "\n",
    "# HuggingFace token\n",
    "hf_token = get(netrc_tokens, \"huggingface.co\", \"\")\n",
    "if !isempty(hf_token)\n",
    "    ENV[\"HF_TOKEN\"] = hf_token\n",
    "    println(\"HF token: found\")\n",
    "else\n",
    "    println(\"HF token: not found (run Python login cell first)\")\n",
    "end\n",
    "\n",
    "# HuggingFace repo ID\n",
    "HF_REPO_ID = get(netrc_tokens, \"hf.repo\", \"\")\n",
    "if !isempty(HF_REPO_ID)\n",
    "    println(\"HF repo: \", HF_REPO_ID)\n",
    "else\n",
    "    println(\"HF repo: not set (add HF_REPO to Colab secrets or set HF_REPO_ID manually)\")\n",
    "end"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# W&B logging via persistent Python subprocess\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nWANDB_PROJECT = \"microgpt-philosophy\"\nWANDB_RUN_ID = \"microgpt-\" * join(rand('a':'z', 6))\n\n# Write a tiny Python helper that reads JSON lines on stdin\nwrite(\"_wandb_log.py\", \"\"\"\nimport wandb, json, sys, os\nproject = os.environ.get(\"WANDB_PROJECT\", \"microgpt-philosophy\")\nrun_id = os.environ.get(\"WANDB_RUN_ID\", None)\nrun = wandb.init(project=project, id=run_id, resume=\"allow\",\n                 config={\"model\": \"microgpt\", \"architecture\": \"1-layer transformer\"})\nprint(f\"W&B run: {run.url}\", flush=True)\nfor line in sys.stdin:\n    line = line.strip()\n    if not line:\n        continue\n    try:\n        data = json.loads(line)\n        wandb.log(data)\n    except Exception as e:\n        print(f\"wandb log error: {e}\", file=sys.stderr, flush=True)\nwandb.finish()\n\"\"\")\n\nwandb_proc = nothing\n\nfunction wandb_init()\n    global wandb_proc, WANDB_PROJECT, WANDB_RUN_ID\n    if !haskey(ENV, \"WANDB_API_KEY\") || isempty(ENV[\"WANDB_API_KEY\"])\n        println(\"W&B: skipped (no API key)\")\n        return\n    end\n    ENV[\"WANDB_PROJECT\"] = WANDB_PROJECT\n    ENV[\"WANDB_RUN_ID\"] = WANDB_RUN_ID\n    wandb_proc = open(`python3 _wandb_log.py`, \"r+\")\n    println(\"W&B: initialized ($WANDB_PROJECT / $WANDB_RUN_ID)\")\nend\n\nfunction wandb_log(; kwargs...)\n    global wandb_proc\n    wandb_proc === nothing && return\n    metrics = Dict(string(k) => v for (k, v) in kwargs)\n    try\n        println(wandb_proc, JSON3.write(metrics))\n        flush(wandb_proc)\n    catch e\n        println(\"W&B log error: $e\")\n    end\nend\n\nfunction wandb_finish()\n    global wandb_proc\n    wandb_proc === nothing && return\n    try close(wandb_proc) catch end\n    wandb_proc = nothing\n    println(\"W&B: run finished\")\nend\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# HuggingFace Hub helpers\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nfunction hf_push(repo_id::String, local_path::String; remote_path::String=\"\")\n    rp = isempty(remote_path) ? basename(local_path) : remote_path\n    run(`huggingface-cli upload $repo_id $local_path $rp`)\n    println(\"Pushed $local_path -> $repo_id/$rp\")\nend\n\nfunction hf_pull(repo_id::String, remote_path::String; local_dir::String=\"checkpoints\")\n    mkpath(local_dir)\n    run(`huggingface-cli download $repo_id $remote_path --local-dir $local_dir`)\n    println(\"Pulled $repo_id/$remote_path -> $local_dir/\")\nend\n\nfunction hf_push_checkpoint(repo_id::String; checkpoint_path::String=\"checkpoints/best_model.json\")\n    isfile(checkpoint_path) || error(\"Checkpoint not found: $checkpoint_path\")\n    hf_push(repo_id, checkpoint_path)\nend\n\nfunction hf_create_repo(repo_id::String)\n    try\n        run(`huggingface-cli repo create $repo_id --type model`)\n        println(\"Created HF repo: $repo_id\")\n    catch\n        println(\"HF repo already exists or creation skipped: $repo_id\")\n    end\nend\n\nprintln(\"W&B + HuggingFace helpers defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Autograd Engine\n",
    "\n",
    "Scalar `Value` type with automatic differentiation.  \n",
    "Each operation creates a node in the computation graph with local gradients stored for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "using Pkg\nPkg.add(\"JSON3\")\n\nusing Random\nusing Printf\nusing JSON3\n\nRandom.seed!(42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Value\n",
    "    data::Float64\n",
    "    grad::Float64\n",
    "    _children::Vector{Value}\n",
    "    _local_grads::Vector{Float64}\n",
    "end\n",
    "\n",
    "Value(x::Real) = Value(Float64(x), 0.0, Value[], Float64[])\n",
    "\n",
    "# --- Core operations ---\n",
    "\n",
    "function Base.:+(a::Value, b::Value)\n",
    "    Value(a.data + b.data, 0.0, [a, b], [1.0, 1.0])\n",
    "end\n",
    "\n",
    "function Base.:*(a::Value, b::Value)\n",
    "    Value(a.data * b.data, 0.0, [a, b], [b.data, a.data])\n",
    "end\n",
    "\n",
    "function Base.:-(a::Value, b::Value)\n",
    "    Value(a.data - b.data, 0.0, [a, b], [1.0, -1.0])\n",
    "end\n",
    "\n",
    "function Base.:/(a::Value, b::Value)\n",
    "    Value(a.data / b.data, 0.0, [a, b], [1.0 / b.data, -a.data / b.data^2])\n",
    "end\n",
    "\n",
    "function Base.:^(a::Value, n::Real)\n",
    "    Value(a.data^n, 0.0, [a], [n * a.data^(n - 1)])\n",
    "end\n",
    "\n",
    "function Base.log(a::Value)\n",
    "    Value(log(a.data), 0.0, [a], [1.0 / a.data])\n",
    "end\n",
    "\n",
    "function Base.exp(a::Value)\n",
    "    e = exp(a.data)\n",
    "    Value(e, 0.0, [a], [e])\n",
    "end\n",
    "\n",
    "function relu(a::Value)\n",
    "    Value(max(0.0, a.data), 0.0, [a], [Float64(a.data > 0)])\n",
    "end\n",
    "\n",
    "# --- Scalar promotion ---\n",
    "\n",
    "Base.:+(a::Value, b::Real) = Value(a.data + b, 0.0, [a], [1.0])\n",
    "Base.:+(a::Real, b::Value) = Value(a + b.data, 0.0, [b], [1.0])\n",
    "Base.:*(a::Value, b::Real) = Value(a.data * b, 0.0, [a], [Float64(b)])\n",
    "Base.:*(a::Real, b::Value) = Value(a * b.data, 0.0, [b], [Float64(a)])\n",
    "Base.:-(a::Value, b::Real) = Value(a.data - b, 0.0, [a], [1.0])\n",
    "Base.:-(a::Real, b::Value) = Value(a - b.data, 0.0, [b], [-1.0])\n",
    "Base.:/(a::Value, b::Real) = Value(a.data / b, 0.0, [a], [1.0 / b])\n",
    "Base.:/(a::Real, b::Value) = Value(a / b.data, 0.0, [b], [-a / b.data^2])\n",
    "Base.:-(a::Value) = Value(-a.data, 0.0, [a], [-1.0])\n",
    "\n",
    "Base.zero(::Type{Value}) = Value(0.0)\n",
    "Base.isless(a::Value, b::Value) = a.data < b.data\n",
    "\n",
    "println(\"Value autograd type defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backward!(loss::Value)\n",
    "    topo = Value[]\n",
    "    visited = Set{UInt64}()\n",
    "    function build_topo(v)\n",
    "        id = objectid(v)\n",
    "        id in visited && return\n",
    "        push!(visited, id)\n",
    "        for child in v._children\n",
    "            build_topo(child)\n",
    "        end\n",
    "        push!(topo, v)\n",
    "    end\n",
    "    build_topo(loss)\n",
    "    loss.grad = 1.0\n",
    "    for v in reverse(topo)\n",
    "        for (child, lg) in zip(v._children, v._local_grads)\n",
    "            child.grad += lg * v.grad\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"backward! defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Dataset â€” Upload Training Data\n\nTraining data is auto-fetched from the GitHub repo during install (cell 0).  \nAlternatively, upload your own  file (one chunk per line) via the Colab file panel.  \nFalls back to built-in philosophy quotes if no file is found."
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 1b. Data Processing Pipeline\n\nUpload raw `.txt` files to the `raw_data/` folder (Colab files panel), then run this cell.  \nIt replicates the local pipeline: strip boilerplate, normalize, lowercase, chunk at sentence boundaries (max 256 chars).  \nOutput is saved as `train.txt` â€” the next cell picks it up automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%shell\ncat > _pipeline.py << 'PIPELINE_EOF'\n\"\"\"Self-contained text processing pipeline for Colab.\nIdentical cleaning + chunking logic to the local pipeline.\"\"\"\n\nimport re, os, sys, random, unicodedata\n\n# â”€â”€ Config â”€â”€\nMAX_CHARS = 256\nMIN_CHARS = 40\nTRAIN_SPLIT = 0.9\nSEED = 42\nINPUT_DIR = \"raw_data\"\nOUTPUT_FILE = \"train.txt\"\nVAL_FILE = \"val.txt\"\n\n# â”€â”€ Gutenberg boilerplate â”€â”€\nGUT_START = re.compile(r\"\\*\\*\\*\\s*START OF (?:THE |THIS )?PROJECT GUTENBERG.*?\\*\\*\\*\", re.IGNORECASE)\nGUT_END = re.compile(r\"\\*\\*\\*\\s*END OF (?:THE |THIS )?PROJECT GUTENBERG.*?\\*\\*\\*\", re.IGNORECASE)\n\n# â”€â”€ MIT Internet Classics Archive â”€â”€\nMIT_HEADER = re.compile(r\"provided by the internet classics archive\\..*?-{6,}\", re.IGNORECASE | re.DOTALL)\nMIT_FOOTER = re.compile(r\"the internet classics archive\\b[^\\n]*(?:web atomics)?[^\\n]*\", re.IGNORECASE)\nMIT_DASHES = re.compile(r\"-{6,}\")\n\n# â”€â”€ Sentence boundary for chunking â”€â”€\nSENTENCE_END = re.compile(r\"[.!?]['\\\"]?\\s+\")\n\ndef strip_gutenberg(text):\n    start = GUT_START.search(text)\n    end_ = GUT_END.search(text)\n    if start:\n        text = text[start.end():]\n    if end_:\n        text = text[:end_.start()]\n    lines = text.split(\"\\n\")\n    cleaned = []\n    skip = start is None\n    for line in lines:\n        s = line.strip()\n        if skip and s.startswith((\"Title:\", \"Author:\", \"Release Date:\",\n                                   \"Language:\", \"Character set\",\n                                   \"Produced by\", \"Updated editions\")):\n            continue\n        if skip and not s:\n            continue\n        skip = False\n        cleaned.append(line)\n    return \"\\n\".join(cleaned)\n\ndef strip_mit_classics(text):\n    text = MIT_HEADER.sub(\"\", text)\n    text = MIT_FOOTER.sub(\"\", text)\n    text = MIT_DASHES.sub(\"\", text)\n    return text\n\ndef normalize_unicode(text):\n    text = unicodedata.normalize(\"NFKD\", text)\n    for old, new in {\n        \"\\u2018\": \"'\", \"\\u2019\": \"'\",\n        \"\\u201c\": '\"', \"\\u201d\": '\"',\n        \"\\u2013\": \"-\", \"\\u2014\": \"-\",\n        \"\\u2026\": \"...\", \"\\u00a0\": \" \",\n        \"\\u00b6\": \"\", \"\\u00a7\": \"\",\n    }.items():\n        text = text.replace(old, new)\n    text = text.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n    return text\n\ndef clean(text):\n    text = strip_gutenberg(text)\n    text = strip_mit_classics(text)\n    text = normalize_unicode(text)\n    text = re.sub(r\"https?://\\S+\", \"\", text)\n    text = re.sub(r\"www\\.\\S+\", \"\", text)\n    text = re.sub(r\"\\S+@\\S+\\.\\S+\", \"\", text)\n    text = text.lower()\n    text = re.sub(r\"[^a-z0-9 .,;:!?'\\\"\\-\\(\\)\\n]\", \" \", text)\n    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n    text = re.sub(r\" {2,}\", \" \", text)\n    text = re.sub(r\" *\\n *\", \"\\n\", text)\n    return text.strip()\n\ndef find_break(text):\n    window = text[:MAX_CHARS]\n    best = -1\n    for m in SENTENCE_END.finditer(window):\n        if m.end() <= MAX_CHARS:\n            best = m.end()\n    if best > MIN_CHARS:\n        return best\n    last_space = window.rfind(\" \")\n    if last_space > MIN_CHARS:\n        return last_space\n    return MAX_CHARS\n\ndef chunk(text):\n    paragraphs = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n    chunks = []\n    for para in paragraphs:\n        para = para.replace(\"\\n\", \" \").strip()\n        if not para:\n            continue\n        if len(para) <= MAX_CHARS:\n            if len(para) >= MIN_CHARS:\n                chunks.append(para)\n            continue\n        remaining = para\n        while remaining:\n            remaining = remaining.strip()\n            if not remaining:\n                break\n            if len(remaining) <= MAX_CHARS:\n                if len(remaining) >= MIN_CHARS:\n                    chunks.append(remaining)\n                break\n            cut = find_break(remaining)\n            piece = remaining[:cut].strip()\n            remaining = remaining[cut:].strip()\n            if len(piece) >= MIN_CHARS:\n                chunks.append(piece)\n    return chunks\n\n# â”€â”€ Main â”€â”€\nif not os.path.isdir(INPUT_DIR):\n    os.makedirs(INPUT_DIR)\n    print(f\"Created {INPUT_DIR}/ â€” upload .txt files there and re-run this cell\")\n    sys.exit(0)\n\nfiles = sorted(f for f in os.listdir(INPUT_DIR) if f.endswith(\".txt\"))\nif not files:\n    print(f\"No .txt files in {INPUT_DIR}/ â€” upload files and re-run\")\n    sys.exit(0)\n\nall_chunks = []\nfor fname in files:\n    path = os.path.join(INPUT_DIR, fname)\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n        raw = f.read()\n    cleaned = clean(raw)\n    chunks = chunk(cleaned)\n    all_chunks.extend(chunks)\n    print(f\"  {fname}: {len(raw):,} chars -> {len(cleaned):,} cleaned -> {len(chunks):,} chunks\")\n\nrandom.seed(SEED)\nrandom.shuffle(all_chunks)\nsplit = int(len(all_chunks) * TRAIN_SPLIT)\ntrain_chunks = all_chunks[:split]\nval_chunks = all_chunks[split:]\n\nwith open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(train_chunks))\nwith open(VAL_FILE, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(val_chunks))\n\nprint(f\"\\n  Total: {len(all_chunks):,} chunks from {len(files)} file(s)\")\nprint(f\"  Output: {len(train_chunks):,} train ({OUTPUT_FILE}), {len(val_chunks):,} val ({VAL_FILE})\")\nPIPELINE_EOF\n\npython3 _pipeline.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Upload training data or use built-in quotes â”€â”€\n\n# Try to load from uploaded file, repo data, or /content/ (from install cell clone)\nTRAINING_DATA = String[]\n\nSEARCH_PATHS = [\n    \"train.txt\",\n    \"aristotle_rhetoric.txt\",\n    \"data.txt\",\n    \"data/aristotle_rhetoric.txt\",\n    \"../data/aristotle_rhetoric.txt\",\n    \"/content/aristotle_rhetoric.txt\",\n    \"/content/voiceclonetake2/data/aristotle_rhetoric.txt\",\n]\n\nfor candidate in SEARCH_PATHS\n    if isfile(candidate)\n        TRAINING_DATA = filter(!isempty, strip.(readlines(candidate)))\n        println(\"Loaded $(length(TRAINING_DATA)) docs from $candidate\")\n        break\n    end\nend\n\n# Fall back to built-in quotes\nif isempty(TRAINING_DATA)\n    TRAINING_DATA = [\n        \"waste no more time arguing about what a good man should be\",\n        \"the happiness of your life depends upon the quality of your thoughts\",\n        \"you have power over your mind not outside events\",\n        \"very little is needed to make a happy life\",\n        \"the soul becomes dyed with the color of its thoughts\",\n        \"when you arise in the morning think of what a privilege it is to be alive\",\n        \"the best revenge is to be unlike him who performed the injury\",\n        \"accept the things to which fate binds you\",\n        \"if it is not right do not do it if it is not true do not say it\",\n        \"look well into thyself there is a source of strength\",\n        \"do every act of your life as though it were the very last act of your life\",\n        \"it is not death that a man should fear but never beginning to live\",\n        \"we suffer more often in imagination than in reality\",\n        \"true happiness is to enjoy the present without anxious dependence upon the future\",\n        \"it is not because things are difficult that we do not dare\",\n        \"it is because we do not dare that they are difficult\",\n        \"luck is what happens when preparation meets opportunity\",\n        \"begin at once to live and count each separate day as a separate life\",\n        \"the whole future lies in uncertainty live immediately\",\n        \"sometimes even to live is an act of courage\",\n        \"the unexamined life is not worth living\",\n        \"i know that i know nothing\",\n        \"wonder is the beginning of wisdom\",\n        \"knowing yourself is the beginning of all wisdom\",\n        \"happiness depends upon ourselves\",\n        \"it is the mark of an educated mind to entertain a thought without accepting it\",\n        \"we are what we repeatedly do excellence then is not an act but a habit\",\n        \"the whole is greater than the sum of its parts\",\n        \"nature does nothing in vain\",\n        \"the journey of a thousand miles begins with a single step\",\n        \"the mind is not a vessel to be filled but a fire to be kindled\",\n    ]\n    println(\"No training file found - using $(length(TRAINING_DATA)) built-in philosophy quotes\")\n    println(\"To use custom data: upload a .txt file or put raw texts in raw_data/ and run the pipeline cell\")\n    println()\n    println(\"Searched paths:\")\n    for p in SEARCH_PATHS\n        println(\"  $p -> $(isfile(p) ? \"found\" : \"not found\")\")\n    end\nend"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Neural Network Primitives + Helpers\n",
    "\n",
    "All operate on vectors of `Value` â€” linear layer, softmax, RMSNorm, plus helper functions for checkpoint support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function linear(x::Vector{Value}, w::Vector{Vector{Value}})\n",
    "    [sum(wi * xi for (wi, xi) in zip(wo, x)) for wo in w]\n",
    "end\n",
    "\n",
    "function softmax_v(logits::Vector{Value})\n",
    "    max_val = maximum(v.data for v in logits)\n",
    "    exps = [exp(v - max_val) for v in logits]\n",
    "    total = sum(exps)\n",
    "    [e / total for e in exps]\n",
    "end\n",
    "\n",
    "function rmsnorm(x::Vector{Value})\n",
    "    ms = sum(xi * xi for xi in x) / length(x)\n",
    "    scale = (ms + 1e-5) ^ (-0.5)\n",
    "    [xi * scale for xi in x]\n",
    "end\n",
    "\n",
    "# Helper: deterministic parameter key ordering\n",
    "function get_param_keys(n_layer::Int)\n",
    "    keys = [\"wte\", \"wpe\", \"lm_head\"]\n",
    "    for i in 0:n_layer-1\n",
    "        append!(keys, [\n",
    "            \"layer$i.attn_wq\", \"layer$i.attn_wk\", \"layer$i.attn_wv\", \"layer$i.attn_wo\",\n",
    "            \"layer$i.mlp_fc1\", \"layer$i.mlp_fc2\"\n",
    "        ])\n",
    "    end\n",
    "    return keys\n",
    "end\n",
    "\n",
    "# Helper: initialize weight matrices\n",
    "function init_matrix(nout::Int, nin::Int; std=0.08)\n",
    "    [[Value(randn() * std) for _ in 1:nin] for _ in 1:nout]\n",
    "end\n",
    "\n",
    "# Helper: flatten state_dict into params vector\n",
    "function flatten_params(state_dict, param_keys)\n",
    "    params = Value[]\n",
    "    for key in param_keys\n",
    "        for row in state_dict[key]\n",
    "            append!(params, row)\n",
    "        end\n",
    "    end\n",
    "    return params\n",
    "end\n",
    "\n",
    "println(\"Neural network primitives + helpers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. GPT Forward Pass\n",
    "\n",
    "Processes one token at a time with KV cache for causal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gpt(token_id::Int, pos_id::Int,\n",
    "             keys::Vector{Vector{Vector{Value}}},\n",
    "             values::Vector{Vector{Vector{Value}}},\n",
    "             state_dict::Dict{String, Vector{Vector{Value}}},\n",
    "             n_layer::Int, n_head::Int, head_dim::Int)\n",
    "\n",
    "    tok_emb = state_dict[\"wte\"][token_id]\n",
    "    pos_emb = state_dict[\"wpe\"][pos_id]\n",
    "    x = [t + p for (t, p) in zip(tok_emb, pos_emb)]\n",
    "    x = rmsnorm(x)\n",
    "\n",
    "    for li in 0:n_layer-1\n",
    "        x_res = x\n",
    "        x = rmsnorm(x)\n",
    "        q = linear(x, state_dict[\"layer$(li).attn_wq\"])\n",
    "        k = linear(x, state_dict[\"layer$(li).attn_wk\"])\n",
    "        v = linear(x, state_dict[\"layer$(li).attn_wv\"])\n",
    "        push!(keys[li+1], k)\n",
    "        push!(values[li+1], v)\n",
    "\n",
    "        x_attn = Value[]\n",
    "        for h in 0:n_head-1\n",
    "            hs = h * head_dim + 1\n",
    "            he = hs + head_dim - 1\n",
    "            q_h = q[hs:he]\n",
    "            k_h = [ki[hs:he] for ki in keys[li+1]]\n",
    "            v_h = [vi[hs:he] for vi in values[li+1]]\n",
    "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in 1:head_dim) / sqrt(Float64(head_dim))\n",
    "                           for t in 1:length(k_h)]\n",
    "            attn_weights = softmax_v(attn_logits)\n",
    "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in 1:length(v_h))\n",
    "                        for j in 1:head_dim]\n",
    "            append!(x_attn, head_out)\n",
    "        end\n",
    "        x = linear(x_attn, state_dict[\"layer$(li).attn_wo\"])\n",
    "        x = [a + b for (a, b) in zip(x, x_res)]\n",
    "\n",
    "        x_res = x\n",
    "        x = rmsnorm(x)\n",
    "        x = linear(x, state_dict[\"layer$(li).mlp_fc1\"])\n",
    "        x = [relu(xi) for xi in x]\n",
    "        x = linear(x, state_dict[\"layer$(li).mlp_fc2\"])\n",
    "        x = [a + b for (a, b) in zip(x, x_res)]\n",
    "    end\n",
    "\n",
    "    logits = linear(x, state_dict[\"lm_head\"])\n",
    "    return logits\n",
    "end\n",
    "\n",
    "println(\"GPT forward pass defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Checkpoint Save/Load\n",
    "\n",
    "Save and load model weights + optimizer state as JSON.  \n",
    "Checkpoints saved to Google Drive persist across Colab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function save_checkpoint(path::String, state_dict, param_keys, uchars, hyperparams;\n",
    "                         m_buf=nothing, v_buf=nothing, step::Int=0,\n",
    "                         lr::Float64=0.01, b1::Float64=0.85, b2::Float64=0.99,\n",
    "                         best_val_loss::Float64=Inf,\n",
    "                         train_losses::Vector{Float64}=Float64[],\n",
    "                         val_losses::Vector{Float64}=Float64[],\n",
    "                         total_steps::Int=0, num_steps_target::Int=0)\n",
    "\n",
    "    sd_data = Dict{String,Any}()\n",
    "    for k in param_keys\n",
    "        sd_data[k] = [[v.data for v in row] for row in state_dict[k]]\n",
    "    end\n",
    "\n",
    "    checkpoint = Dict{String,Any}(\n",
    "        \"uchars\" => [string(c) for c in uchars],\n",
    "        \"hyperparams\" => hyperparams,\n",
    "        \"state_dict\" => sd_data,\n",
    "        \"optimizer\" => Dict{String,Any}(\n",
    "            \"m_buf\" => m_buf === nothing ? Float64[] : collect(m_buf),\n",
    "            \"v_buf\" => v_buf === nothing ? Float64[] : collect(v_buf),\n",
    "            \"step\" => step,\n",
    "            \"lr\" => lr,\n",
    "            \"beta1\" => b1,\n",
    "            \"beta2\" => b2\n",
    "        ),\n",
    "        \"training\" => Dict{String,Any}(\n",
    "            \"best_val_loss\" => best_val_loss,\n",
    "            \"train_losses\" => train_losses,\n",
    "            \"val_losses\" => val_losses,\n",
    "            \"total_steps_completed\" => total_steps,\n",
    "            \"num_steps_target\" => num_steps_target\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mkpath(dirname(path))\n",
    "    open(path, \"w\") do f\n",
    "        JSON3.write(f, checkpoint)\n",
    "    end\n",
    "    vl_str = best_val_loss == Inf ? \"Inf\" : @sprintf(\"%.4f\", best_val_loss)\n",
    "    println(\"Checkpoint saved: $path (step $step, best_val_loss=$vl_str)\")\n",
    "end\n",
    "\n",
    "function load_checkpoint(path::String)\n",
    "    println(\"Loading checkpoint from $path ...\")\n",
    "    raw = JSON3.read(read(path, String))\n",
    "\n",
    "    uchars = [only(String(s)) for s in raw[\"uchars\"]]\n",
    "    BOS = length(uchars) + 1\n",
    "    vocab_size = BOS\n",
    "\n",
    "    hp = raw[\"hyperparams\"]\n",
    "    n_layer = Int(hp[\"n_layer\"])\n",
    "    n_embd = Int(hp[\"n_embd\"])\n",
    "    block_size = Int(hp[\"block_size\"])\n",
    "    n_head = Int(hp[\"n_head\"])\n",
    "    head_dim = n_embd Ã· n_head\n",
    "\n",
    "    state_dict = Dict{String, Vector{Vector{Value}}}()\n",
    "    for (key, matrix) in pairs(raw[\"state_dict\"])\n",
    "        state_dict[string(key)] = [[Value(Float64(v)) for v in row] for row in matrix]\n",
    "    end\n",
    "\n",
    "    opt = raw[\"optimizer\"]\n",
    "    m_buf = Float64.(collect(opt[\"m_buf\"]))\n",
    "    v_buf = Float64.(collect(opt[\"v_buf\"]))\n",
    "    step = Int(opt[\"step\"])\n",
    "    lr = Float64(opt[\"lr\"])\n",
    "    b1 = Float64(opt[\"beta1\"])\n",
    "    b2 = Float64(opt[\"beta2\"])\n",
    "\n",
    "    trn = raw[\"training\"]\n",
    "    best_val_loss = Float64(trn[\"best_val_loss\"])\n",
    "    train_losses = Float64.(collect(trn[\"train_losses\"]))\n",
    "    val_losses = Float64.(collect(trn[\"val_losses\"]))\n",
    "    total_steps = Int(trn[\"total_steps_completed\"])\n",
    "    num_steps_target = Int(trn[\"num_steps_target\"])\n",
    "\n",
    "    println(\"  vocab=$vocab_size, embd=$n_embd, layers=$n_layer, step=$step\")\n",
    "\n",
    "    return (;\n",
    "        state_dict, uchars, BOS, vocab_size,\n",
    "        n_layer, n_embd, block_size, n_head, head_dim,\n",
    "        m_buf, v_buf, step, lr, b1, b2,\n",
    "        best_val_loss, train_losses, val_losses,\n",
    "        total_steps, num_steps_target\n",
    "    )\n",
    "end\n",
    "\n",
    "println(\"Checkpoint save/load defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Setup â€” Dataset, Tokenizer, Parameters\n",
    "\n",
    "Character-level tokenizer with a BOS token. 90/10 train/val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Dataset with train/val split â”€â”€\ndocs = shuffle(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\nprintln(\"train: $(length(train_docs)) docs, val: $(length(val_docs)) docs\")\n\n# â”€â”€ Tokenizer â”€â”€\nuchars = sort(unique(join(docs)))\nBOS = length(uchars) + 1\nvocab_size = BOS\nprintln(\"vocab size: $vocab_size ($(length(uchars)) chars + BOS)\")\n\n# â”€â”€ Hyperparameters â”€â”€\nn_layer    = 1\nn_embd     = 16\nblock_size = 256\nn_head     = 4\nhead_dim   = n_embd Ã· n_head\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# â”€â”€ Initialize parameters â”€â”€\nstate_dict = Dict{String, Vector{Vector{Value}}}()\nstate_dict[\"wte\"]     = init_matrix(vocab_size, n_embd)\nstate_dict[\"wpe\"]     = init_matrix(block_size, n_embd)\nstate_dict[\"lm_head\"] = init_matrix(vocab_size, n_embd)\nfor i in 0:n_layer-1\n    state_dict[\"layer$i.attn_wq\"]  = init_matrix(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wk\"]  = init_matrix(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wv\"]  = init_matrix(n_embd, n_embd)\n    state_dict[\"layer$i.attn_wo\"]  = init_matrix(n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc1\"]  = init_matrix(4 * n_embd, n_embd)\n    state_dict[\"layer$i.mlp_fc2\"]  = init_matrix(n_embd, 4 * n_embd)\nend\n\nparam_keys = get_param_keys(n_layer)\nparams = flatten_params(state_dict, param_keys)\nprintln(\"num params: $(length(params))\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Loop with Validation + Best-Model Checkpointing\n",
    "\n",
    "Adam optimizer with linear LR decay.  \n",
    "Validates every 50 steps, saves `best_model.json` when val loss improves.  \n",
    "Periodic checkpoints every 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for doc in val_docs\n",
    "        tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n",
    "        n = min(block_size, length(tokens) - 1)\n",
    "        kv_keys = [Vector{Vector{Value}}() for _ in 1:n_layer]\n",
    "        kv_vals = [Vector{Vector{Value}}() for _ in 1:n_layer]\n",
    "        for pos in 1:n\n",
    "            token_id = tokens[pos]\n",
    "            target_id = tokens[pos + 1]\n",
    "            logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n",
    "            probs = softmax_v(logits)\n",
    "            total_loss += -log(probs[target_id]).data\n",
    "            total_tokens += 1\n",
    "        end\n",
    "    end\n",
    "    return total_loss / max(total_tokens, 1)\n",
    "end\n",
    "\n",
    "println(\"compute_val_loss defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Adam optimizer buffers â”€â”€\nlr, b1, b2, eps = 0.01, 0.85, 0.99, 1e-8\nm_buf = zeros(length(params))\nv_buf = zeros(length(params))\n\nbest_val_loss = Inf\ntrain_loss_history = Float64[]\nval_loss_history = Float64[]\n\n# â”€â”€ Initialize W&B logging (if API key is set) â”€â”€\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\n# â”€â”€ Training loop â”€â”€\n# Dynamic steps: ~3 epochs over training data, min 1000, max 50000\nNUM_EPOCHS = 3\nnum_steps = clamp(NUM_EPOCHS * length(train_docs), 1000, 50000)\nprintln(\"--- training $num_steps steps (~$(NUM_EPOCHS) epochs over $(length(train_docs)) docs) ---\")\nt_start = time()\n\nfor step in 1:num_steps\n    doc = train_docs[mod1(step, length(train_docs))]\n    tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n    n = min(block_size, length(tokens) - 1)\n\n    # Forward pass\n    kv_keys = [Vector{Vector{Value}}() for _ in 1:n_layer]\n    kv_vals = [Vector{Vector{Value}}() for _ in 1:n_layer]\n    losses = Value[]\n    for pos in 1:n\n        token_id  = tokens[pos]\n        target_id = tokens[pos + 1]\n        logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n        probs  = softmax_v(logits)\n        push!(losses, -log(probs[target_id]))\n    end\n    loss = (1 / n) * sum(losses)\n    push!(train_loss_history, loss.data)\n\n    # Backward pass\n    backward!(loss)\n\n    # Adam update with linear LR decay\n    lr_t = lr * (1 - (step - 1) / num_steps)\n    for (i, p) in enumerate(params)\n        m_buf[i] = b1 * m_buf[i] + (1 - b1) * p.grad\n        v_buf[i] = b2 * v_buf[i] + (1 - b2) * p.grad^2\n        m_hat = m_buf[i] / (1 - b1^step)\n        v_hat = v_buf[i] / (1 - b2^step)\n        p.data -= lr_t * m_hat / (sqrt(v_hat) + eps)\n        p.grad = 0.0\n    end\n\n    # Validate + checkpoint every 50 steps\n    if step % 50 == 0\n        val_loss = compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n        push!(val_loss_history, val_loss)\n        elapsed = time() - t_start\n\n        # Log to W&B\n        wandb_log(; step=step, train_loss=loss.data, val_loss=val_loss, lr=lr_t)\n\n        improved = \"\"\n        if val_loss < best_val_loss\n            best_val_loss = val_loss\n            save_checkpoint(\"checkpoints/best_model.json\", state_dict, param_keys, uchars, hyperparams;\n                m_buf=m_buf, v_buf=v_buf, step=step,\n                lr=lr, b1=b1, b2=b2,\n                best_val_loss=best_val_loss,\n                train_losses=train_loss_history, val_losses=val_loss_history,\n                total_steps=step, num_steps_target=num_steps)\n            improved = \" << new best!\"\n        end\n\n        @printf(\"step %4d / %4d | train %.4f | val %.4f | %.1fs%s\\n\",\n                step, num_steps, loss.data, val_loss, elapsed, improved)\n    elseif step % 10 == 0\n        elapsed = time() - t_start\n        @printf(\"step %4d / %4d | train %.4f | %.1fs\\n\", step, num_steps, loss.data, elapsed)\n    end\n\n    # Periodic checkpoint every 200 steps\n    if step % 200 == 0\n        save_checkpoint(\"checkpoints/checkpoint_step$(step).json\", state_dict, param_keys, uchars, hyperparams;\n            m_buf=m_buf, v_buf=v_buf, step=step,\n            lr=lr, b1=b1, b2=b2,\n            best_val_loss=best_val_loss,\n            train_losses=train_loss_history, val_losses=val_loss_history,\n            total_steps=step, num_steps_target=num_steps)\n    end\nend\n\nelapsed = time() - t_start\n@printf(\"\\ntraining complete in %.1f seconds\\n\", elapsed)\n\n# Finish W&B run\nwandb_finish()\n\n# Final save\nsave_checkpoint(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    m_buf=m_buf, v_buf=v_buf, step=num_steps,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=num_steps, num_steps_target=num_steps)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Inference â€” Hallucinated Philosophy\n",
    "\n",
    "Generate new philosophy-like text using temperature-controlled sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_text(state_dict, uchars, BOS, n_layer, n_head, head_dim, block_size;\n",
    "                       temperature=0.8, max_tokens=128)\n",
    "    kv_keys = [Vector{Vector{Value}}() for _ in 1:n_layer]\n",
    "    kv_vals = [Vector{Vector{Value}}() for _ in 1:n_layer]\n",
    "    token_id = BOS\n",
    "    sample = Char[]\n",
    "    limit = min(max_tokens, block_size)\n",
    "    for pos in 1:limit\n",
    "        logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n",
    "        scaled = [l / temperature for l in logits]\n",
    "        probs = softmax_v(scaled)\n",
    "        weights = [p.data for p in probs]\n",
    "        r = rand()\n",
    "        cum = 0.0\n",
    "        token_id = 1\n",
    "        for (idx, w) in enumerate(weights)\n",
    "            cum += w\n",
    "            if r <= cum\n",
    "                token_id = idx\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        token_id == BOS && break\n",
    "        push!(sample, uchars[token_id])\n",
    "    end\n",
    "    return String(sample)\n",
    "end\n",
    "\n",
    "println(\"--- inference (hallucinated philosophy) ---\")\n",
    "for i in 1:20\n",
    "    text = generate_text(state_dict, uchars, BOS, n_layer, n_head, head_dim, block_size; temperature=0.8)\n",
    "    @printf(\"sample %2d: %s\\n\", i, text)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8a. Push Model to HuggingFace Hub\n\nPush your trained checkpoint to HuggingFace for persistence across Colab sessions.  \nSet `HF_REPO_ID` in the login cell above (e.g. `\"yourusername/microgpt-philosophy\"`).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Push checkpoint to HuggingFace Hub\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    # Create repo if it doesn't exist yet\n    hf_create_repo(HF_REPO_ID)\n\n    # Push best model checkpoint\n    if isfile(\"checkpoints/best_model.json\")\n        hf_push_checkpoint(HF_REPO_ID; checkpoint_path=\"checkpoints/best_model.json\")\n    else\n        println(\"No best_model.json found â€” train first!\")\n    end\n\n    # Also push final model if it exists\n    if isfile(\"checkpoints/final_model.json\")\n        hf_push(HF_REPO_ID, \"checkpoints/final_model.json\")\n    end\n\n    println(\"\\nDone! View your model at: https://huggingface.co/$HF_REPO_ID\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8b. Pull Checkpoint from HuggingFace Hub\n\nDownload a previously pushed checkpoint to resume training in a new Colab session.  \nRun this before the Resume Training cell below.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Pull checkpoint from HuggingFace to resume training\n# Make sure HF_REPO_ID is set in the login cell above\n\nif @isdefined(HF_REPO_ID) && !isempty(HF_REPO_ID)\n    mkpath(\"checkpoints\")\n    hf_pull(HF_REPO_ID, \"best_model.json\"; local_dir=\"checkpoints\")\n    println(\"\\nReady to resume from checkpoints/best_model.json\")\n    println(\"Run the 'Resume Training' cell below.\")\nelse\n    println(\"Set HF_REPO_ID in the login cell (e.g. \\\"yourusername/microgpt-philosophy\\\")\")\nend",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Resume Training from Checkpoint\n",
    "\n",
    "Load a saved checkpoint and continue training for more steps.  \n",
    "Skip this cell if you're training from scratch above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ Load checkpoint â”€â”€\n# Change the path to load a different checkpoint\nRESUME_FROM = \"checkpoints/best_model.json\"\nEXTRA_STEPS = clamp(length(train_docs), 500, 25000)  # ~1 extra epoch\n\nckpt = load_checkpoint(RESUME_FROM)\nstate_dict = ckpt.state_dict\nuchars = ckpt.uchars\nBOS = ckpt.BOS\nn_layer = ckpt.n_layer\nn_embd = ckpt.n_embd\nblock_size = ckpt.block_size\nn_head = ckpt.n_head\nhead_dim = ckpt.head_dim\n\nhyperparams = Dict{String,Any}(\n    \"n_layer\" => n_layer, \"n_embd\" => n_embd,\n    \"block_size\" => block_size, \"n_head\" => n_head\n)\n\n# Reconstruct dataset and split\ndocs = shuffle(TRAINING_DATA)\nsplit_idx = max(1, Int(floor(0.9 * length(docs))))\ntrain_docs = docs[1:split_idx]\nval_docs = docs[split_idx+1:end]\nif isempty(val_docs)\n    val_docs = docs[max(1, end-4):end]\n    train_docs = docs[1:max(1, end-5)]\nend\n\nparam_keys = get_param_keys(n_layer)\nparams = flatten_params(state_dict, param_keys)\n\n# Restore optimizer\nlr = ckpt.lr; b1 = ckpt.b1; b2 = ckpt.b2; eps = 1e-8\nm_buf = length(ckpt.m_buf) == length(params) ? copy(ckpt.m_buf) : zeros(length(params))\nv_buf = length(ckpt.v_buf) == length(params) ? copy(ckpt.v_buf) : zeros(length(params))\n\nstart_step = ckpt.step + 1\nend_step = ckpt.step + EXTRA_STEPS\nbest_val_loss = ckpt.best_val_loss\ntrain_loss_history = copy(ckpt.train_losses)\nval_loss_history = copy(ckpt.val_losses)\n\n# â”€â”€ Initialize W&B logging (if API key is set) â”€â”€\nif haskey(ENV, \"WANDB_API_KEY\") && !isempty(ENV[\"WANDB_API_KEY\"])\n    wandb_init()\nend\n\nprintln(\"\\nResuming from step $(ckpt.step) -> training to step $end_step\")\nprintln(\"Best val loss so far: $(round(best_val_loss, digits=4))\")\nt_start = time()\n\nfor step in start_step:end_step\n    doc = train_docs[mod1(step, length(train_docs))]\n    tokens = vcat([BOS], [findfirst(==(ch), uchars) for ch in doc], [BOS])\n    n = min(block_size, length(tokens) - 1)\n\n    kv_keys = [Vector{Vector{Value}}() for _ in 1:n_layer]\n    kv_vals = [Vector{Vector{Value}}() for _ in 1:n_layer]\n    losses = Value[]\n    for pos in 1:n\n        token_id  = tokens[pos]\n        target_id = tokens[pos + 1]\n        logits = gpt(token_id, pos, kv_keys, kv_vals, state_dict, n_layer, n_head, head_dim)\n        probs  = softmax_v(logits)\n        push!(losses, -log(probs[target_id]))\n    end\n    loss = (1 / n) * sum(losses)\n    push!(train_loss_history, loss.data)\n\n    backward!(loss)\n\n    lr_t = lr * (1 - (step - 1) / end_step)\n    for (i, p) in enumerate(params)\n        m_buf[i] = b1 * m_buf[i] + (1 - b1) * p.grad\n        v_buf[i] = b2 * v_buf[i] + (1 - b2) * p.grad^2\n        m_hat = m_buf[i] / (1 - b1^step)\n        v_hat = v_buf[i] / (1 - b2^step)\n        p.data -= lr_t * m_hat / (sqrt(v_hat) + eps)\n        p.grad = 0.0\n    end\n\n    if step % 50 == 0\n        val_loss = compute_val_loss(val_docs, uchars, BOS, block_size, state_dict, n_layer, n_head, head_dim)\n        push!(val_loss_history, val_loss)\n        elapsed = time() - t_start\n\n        # Log to W&B\n        wandb_log(; step=step, train_loss=loss.data, val_loss=val_loss, lr=lr_t)\n\n        improved = \"\"\n        if val_loss < best_val_loss\n            best_val_loss = val_loss\n            save_checkpoint(\"checkpoints/best_model.json\", state_dict, param_keys, uchars, hyperparams;\n                m_buf=m_buf, v_buf=v_buf, step=step,\n                lr=lr, b1=b1, b2=b2,\n                best_val_loss=best_val_loss,\n                train_losses=train_loss_history, val_losses=val_loss_history,\n                total_steps=step, num_steps_target=end_step)\n            improved = \" << new best!\"\n        end\n        @printf(\"step %4d / %4d | train %.4f | val %.4f | %.1fs%s\\n\",\n                step, end_step, loss.data, val_loss, elapsed, improved)\n    elseif step % 10 == 0\n        elapsed = time() - t_start\n        @printf(\"step %4d / %4d | train %.4f | %.1fs\\n\", step, end_step, loss.data, elapsed)\n    end\nend\n\nelapsed = time() - t_start\n@printf(\"\\nresume training complete in %.1f seconds\\n\", elapsed)\n\n# Finish W&B run\nwandb_finish()\n\nsave_checkpoint(\"checkpoints/final_model.json\", state_dict, param_keys, uchars, hyperparams;\n    m_buf=m_buf, v_buf=v_buf, step=end_step,\n    lr=lr, b1=b1, b2=b2,\n    best_val_loss=best_val_loss,\n    train_losses=train_loss_history, val_losses=val_loss_history,\n    total_steps=end_step, num_steps_target=end_step)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Download Checkpoint\n",
    "\n",
    "Download the best model checkpoint to use with the inference server.  \n",
    "In Colab, use the Files panel (left sidebar) to download, or copy to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List saved checkpoints\n",
    "if isdir(\"checkpoints\")\n",
    "    files = readdir(\"checkpoints\")\n",
    "    println(\"Saved checkpoints:\")\n",
    "    for f in files\n",
    "        path = joinpath(\"checkpoints\", f)\n",
    "        size_kb = round(filesize(path) / 1024, digits=1)\n",
    "        println(\"  $path ($(size_kb) KB)\")\n",
    "    end\n",
    "else\n",
    "    println(\"No checkpoints directory found. Train first!\")\n",
    "end"
   ]
  }
 ]
}